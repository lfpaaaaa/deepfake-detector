# Testing Strategy - Deepfake Detector

**Project**: Deepfake Detector  
**Version**: 3.0  
**Last Updated**: October 26, 2025  
**Document Owner**: Xiyu Guan

---

## ğŸ“‹ Table of Contents

1. [Overview](#overview)
2. [Testing Philosophy](#testing-philosophy)
3. [Test Types and Scope](#test-types-and-scope)
4. [Testing Levels](#testing-levels)
5. [Test Automation Strategy](#test-automation-strategy)
6. [Test Cycles](#test-cycles)
7. [Test Environment](#test-environment)
8. [Tools and Framework](#tools-and-framework)
9. [Documentation Structure](#documentation-structure)
10. [Continuous Integration](#continuous-integration)
11. [Metrics and Reporting](#metrics-and-reporting)

---

## Overview

### Purpose
This document outlines the comprehensive testing strategy for the Deepfake Detector project, including:
- Test types and coverage
- Automation approach
- Test cycles and planning
- Quality metrics and criteria
- CI/CD integration

### Scope
The testing strategy covers:
- âœ… Unit testing of individual components
- âœ… Integration testing of API endpoints
- âœ… Authentication and security testing
- âœ… Model loading and inference testing
- âœ… End-to-end user workflows
- âœ… Performance and load testing
- âœ… Cross-browser compatibility (web UI)

---

## Testing Philosophy

### Core Principles

1. **Test Early, Test Often**
   - Write tests alongside code
   - Run tests before every commit
   - Catch bugs early in development

2. **Automate Everything Possible**
   - Prioritize automated tests over manual
   - Use CI/CD to run tests automatically
   - Generate automated reports

3. **Test What Matters**
   - Focus on critical user journeys
   - Test edge cases and error conditions
   - Don't test framework code

4. **Maintainable Tests**
   - Clear, readable test code
   - Proper test isolation
   - Reusable fixtures and utilities

5. **Evidence-Based Testing**
   - Document test results with evidence
   - Capture screenshots and logs
   - Track metrics over time

---

## Test Types and Scope

### 1. Unit Tests
**Objective**: Test individual functions and classes in isolation

**Scope**:
- Module imports
- Configuration validation
- Utility functions
- Data models
- Business logic

**Tools**: pytest  
**Coverage Target**: 70%  
**Automated**: 100%

---

### 2. Integration Tests
**Objective**: Test interaction between components

**Scope**:
- API endpoints
- Database operations
- Authentication flow
- Model adapter integration
- File handling

**Tools**: pytest + FastAPI TestClient  
**Coverage Target**: 80%  
**Automated**: 90%

---

### 3. End-to-End Tests
**Objective**: Test complete user workflows

**Scope**:
- User registration â†’ Login â†’ Detection â†’ View History
- Image detection workflow
- Video detection workflow
- Error handling and recovery

**Tools**: Manual + pytest  
**Coverage Target**: Key workflows  
**Automated**: 50%

---

### 4. Security Tests
**Objective**: Verify security controls

**Scope**:
- Authentication and authorization
- JWT token validation
- Input validation and sanitization
- Protection against common attacks (XSS, SQL injection)
- Secure file handling

**Tools**: Manual + security scanners (Trivy)  
**Coverage Target**: All security features  
**Automated**: 70%

---

### 5. Performance Tests
**Objective**: Measure system performance

**Scope**:
- API response times
- Model inference time
- Concurrent request handling
- Resource usage (CPU, memory)

**Tools**: Manual timing + pytest markers  
**Coverage Target**: Critical operations  
**Automated**: 30%

---

## Testing Levels

### Pyramid Model

```
          â•±â•²
         â•± E2E â•²         â† 10% (Manual-heavy)
        â•±â”€â”€â”€â”€â”€â”€â”€â”€â•²
       â•±Integrationâ•²     â† 30% (API, Auth, Models)
      â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•²
     â•±   Unit Tests   â•²  â† 60% (Core logic, Utils)
    â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•²
```

**Rationale**:
- Unit tests are fast, cheap, and cover most code
- Integration tests verify component interaction
- E2E tests validate critical user journeys

---

## Test Automation Strategy

### Automation Priority

**High Priority (100% Automated)**:
- Unit tests
- API endpoint tests
- Authentication tests
- CI/CD pipeline tests

**Medium Priority (70% Automated)**:
- Integration tests with external components
- Model loading tests
- Error handling tests

**Low Priority (30% Automated)**:
- UI tests
- Performance tests
- Manual exploratory testing

---

### Automated Test Execution

**Local Development** (Windows):
```bash
# Navigate to project
cd c:\Users\21402\Desktop\deepfake-detector

# Run all tests with coverage
pytest tests/ -v --cov=app --cov-report=html

# Run specific types
pytest tests/ -v -m unit           # Unit tests only
pytest tests/ -v -m integration    # Integration tests only

# View coverage report
# Open htmlcov/index.html in browser
```

**Expected Local Results**:
- â±ï¸ Execution time: ~12 seconds
- âœ… 67 tests passed, 2 skipped
- ğŸ“Š Coverage: 70%

**CI/CD Pipeline** (GitHub Actions):
- **Platform**: Ubuntu Latest
- **Triggers**: Push to `main`, Pull Requests
- **Duration**: ~4 minutes (full pipeline)
- **Jobs**: 5 parallel jobs
  1. Code Quality (flake8, black, isort)
  2. Security Scan (Trivy)
  3. Quick Validation (syntax check)
  4. Unit Tests (pytest + coverage)
  5. Docker Build + Config Validation

**CI Results**:
- âœ… All 67 tests pass on Ubuntu
- âœ… No security vulnerabilities
- âœ… 70% code coverage
- âœ… Docker image builds successfully

**Dual Testing Benefits**:
- ğŸš€ **Local**: Fast feedback for development (~12s)
- ğŸ›¡ï¸ **CI**: Cross-platform validation + security checks (~4m)
- ğŸ“Š **Both**: Comprehensive quality assurance

---

## Test Cycles

### Cycle Structure

Each test cycle focuses on specific features and includes:
1. **Test Plan**: Defines scope, schedule, and selected tests
2. **Test Execution**: Run tests and document results
3. **Test Report**: Summarize results, bugs, and recommendations
4. **Review**: Team review and sign-off

---

### Planned Cycles

#### **Cycle 1: Basic Functionality** âœ… Complete
**Focus**: Core system setup and authentication  
**Duration**: 2 days (Oct 25-26, 2025)  
**Status**: âœ… Complete

**Key Areas**:
- âœ… Module imports and configuration (3 tests)
- âœ… User registration and login (10 tests)
- âœ… JWT authentication (10 tests)
- âœ… API endpoint accessibility (10 tests)
- âœ… CI/CD pipeline (5 jobs)
- âœ… History management (16 tests)
- âœ… Model adapters (26 tests)

**Test Results**:
- **Tests Executed**: 67 automated tests
- **Pass Rate**: 100% (67 passed, 0 failed)
- **Code Coverage**: 70% (target: â‰¥60%)
- **Test Environment**: Local (Windows) + CI (Ubuntu)
- **Bugs Found**: 6 (all fixed)

**Documentation**:
- [Test Plan](test_plans/cycle_1_basic_functionality.md) - V2.0 âœ…
- [Test Report](test_reports/cycle_1_report.md) - V2.0 âœ…

---

#### **Cycle 2: Model Integration & End-to-End Testing** âœ… Complete
**Focus**: Model loading, detection workflows, and comprehensive E2E testing  
**Duration**: 1 day (6.5 hours, October 26, 2025)  
**Status**: âœ… **Complete**

**Key Areas**:
- âœ… TruFor image detection (JPG, PNG, multiple sizes)
- âœ… DeepfakeBench video analysis (12 models, MP4, WEBM)
- âœ… Multi-format support and large file handling (80MB+)
- âœ… Edge cases (corrupted files, special characters, concurrent uploads)
- âœ… Performance testing (API response times, detection speed)
- âœ… Security testing (SQL injection, token validation, data isolation)
- âœ… User workflows (authentication, history, reports, mobile UI)

**Test Results**:
- **Tests Executed**: 42/43 (98% coverage)
- **Pass Rate**: 88% (37 passed, 5 partial pass, 1 failed)
- **Bugs Found**: 4 (2 high, 2 medium)
- **UX Issues**: 3 (low priority enhancements)
- **Duration**: 6.5 hours (ahead of schedule)

**Key Achievements**:
- âœ… Core detection functionality: 100% verified
- âœ… All 12 DeepfakeBench models tested
- âœ… Large file handling (80MB+ videos) working
- âœ… User data isolation verified
- âœ… Mobile responsive design excellent
- âœ… Concurrent processing capabilities confirmed

**Exit Criteria Met**: âœ… Yes
- âœ… Test execution â‰¥90%: **98%** achieved
- âœ… Pass rate â‰¥70%: **88%** achieved
- âš ï¸ 2 high-severity bugs require fixes before production

---

#### **Cycle 3: Edge Cases & Performance** ğŸŸ¡ Planned
**Focus**: Error handling and performance  
**Duration**: 4 days  
**Status**: Planned

**Key Areas**:
- Invalid file formats
- Large file handling
- Concurrent requests
- Error recovery
- Performance benchmarks

**Entry Criteria**:
- Cycle 2 complete with â‰¥ 90% pass rate
- All critical bugs fixed

---

#### **Cycle 4: Security & UI** ğŸŸ¡ Planned
**Focus**: Security testing and UI validation  
**Duration**: 3 days  
**Status**: Planned

**Key Areas**:
- XSS and injection attacks
- Authentication bypass attempts
- UI responsiveness (mobile/desktop)
- Cross-browser compatibility
- Accessibility

**Entry Criteria**:
- Cycle 3 complete
- Security scan results reviewed

---

## Test Environment

### Development Environment
- **OS**: Windows/Linux/Mac
- **Python**: 3.11
- **Dependencies**: From `configs/requirements.txt`
- **Local Server**: `uvicorn` or Docker

---

### Test Environment
- **OS**: Ubuntu Linux (CI) + Windows/Mac (local)
- **Docker**: Latest Docker Desktop
- **Browser**: Chrome, Firefox (latest versions)
- **Model Weights**: Optional (some tests require them)

---

### CI/CD Environment
- **Platform**: GitHub Actions
- **OS**: Ubuntu Latest
- **Python**: 3.11
- **Docker**: Built-in GitHub Actions

---

## Tools and Framework

### Testing Tools

| Tool | Purpose | Usage |
|------|---------|-------|
| **pytest** | Test framework | All automated tests |
| **pytest-cov** | Coverage measurement | Code coverage reports |
| **pytest-asyncio** | Async test support | FastAPI endpoints |
| **FastAPI TestClient** | API testing | HTTP endpoint tests |
| **Trivy** | Security scanning | Vulnerability detection |
| **Black** | Code formatting | Code quality checks |
| **Flake8** | Linting | Code quality checks |
| **isort** | Import sorting | Code quality checks |

---

### Test Configuration

**pytest.ini**:
```ini
[pytest]
python_files = test_*.py *_test.py
testpaths = tests
markers =
    unit: Unit tests
    integration: Integration tests
    slow: Slow running tests
    requires_models: Tests needing model weights
```

**Coverage Configuration**:
```ini
[coverage:run]
source = app
omit = */tests/*

[coverage:report]
precision = 2
show_missing = True
```

---

## Documentation Structure

### Testing Documents

```
docs/testing/
â”œâ”€â”€ README.md                    â† This file (Strategy overview)
â”œâ”€â”€ TEST_CASES.md                â† All test cases (28 cases)
â”œâ”€â”€ test_plans/                  â† Test cycle plans
â”‚   â”œâ”€â”€ cycle_1_basic_functionality.md
â”‚   â”œâ”€â”€ cycle_2_model_integration.md (planned)
â”‚   â”œâ”€â”€ cycle_3_edge_cases.md (planned)
â”‚   â””â”€â”€ cycle_4_security_ui.md (planned)
â””â”€â”€ test_reports/                â† Test execution reports
    â”œâ”€â”€ cycle_1_report.md
    â”œâ”€â”€ cycle_2_report.md (planned)
    â”œâ”€â”€ cycle_3_report.md (planned)
    â””â”€â”€ cycle_4_report.md (planned)
```

---

### Test Code Structure

```
tests/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ test_basic.py           â† Unit tests (imports, config)
â”œâ”€â”€ test_integration.py     â† Integration tests (API, auth)
â”œâ”€â”€ test_model_loading.py   â† Model loading tests
â”œâ”€â”€ test_api_endpoints.py   â† Manual API testing script
â””â”€â”€ conftest.py            â† Shared fixtures (future)
```

---

## Continuous Integration

### GitHub Actions Workflows

#### **CI Tests** (`.github/workflows/ci.yml`)
Comprehensive test suite including:
- âœ… Code quality checks (flake8, black, isort)
- âœ… Security scanning (Trivy)
- âœ… Docker build test
- âœ… Unit and integration tests
- âœ… Coverage report generation
- âœ… Configuration validation

**Triggered on**:
- Push to `main` or `dev` branches
- Pull requests to `main`

---

#### **Quick Tests** (`.github/workflows/quick-test.yml`)
Fast feedback loop:
- âœ… Python syntax validation
- âœ… Configuration file checks
- âœ… Basic import tests

**Triggered on**:
- Every push
- Draft PRs

---

### CI Pipeline Flow

```
1. Code Push
     â†“
2. Checkout Code
     â†“
3. Setup Python 3.11
     â†“
4. Install Dependencies
     â†“
5. Run Linters (flake8, black, isort)
     â†“
6. Security Scan (Trivy)
     â†“
7. Run Unit Tests (pytest -m unit)
     â†“
8. Run Integration Tests (pytest -m integration)
     â†“
9. Generate Coverage Report
     â†“
10. Upload Results
     â†“
11. âœ… Pass or âŒ Fail
```

---

## Metrics and Reporting

### Quality Metrics

| Metric | Target | Current | Status |
|--------|--------|---------|--------|
| **Code Coverage** | â‰¥ 70% | _TBD_ | ğŸŸ¡ |
| **Test Pass Rate** | â‰¥ 95% | _TBD_ | ğŸŸ¡ |
| **Critical Bugs** | 0 | _TBD_ | ğŸŸ¡ |
| **CI Build Success** | â‰¥ 90% | _TBD_ | ğŸŸ¡ |
| **Test Automation** | â‰¥ 80% | ~70% | ğŸŸ¡ |

---

### Reporting

**After Each Test Cycle**:
1. âœ… Complete test report with evidence
2. âœ… Coverage report (HTML + XML)
3. âœ… Bug summary and status
4. âœ… Metrics dashboard update
5. âœ… Lessons learned document

**Continuous**:
- CI/CD results on every push
- Coverage trend tracking
- Test execution time trends

---

## Test Data Management

### Test Users
- Generated dynamically with timestamps
- Cleaned up after test execution
- Format: `test_user_{timestamp}`

### Test Files
- Sample images (authentic and fake)
- Sample videos (various lengths)
- Invalid file formats for error testing
- Stored in `tests/test_data/` (gitignored)

### Model Weights
- Not committed to repository
- Downloaded separately from Google Drive
- Optional for many tests (mocked when needed)

---

## Roles and Responsibilities

| Role | Responsibilities | Owner |
|------|------------------|-------|
| **Test Lead** | Overall strategy, planning, review | Xiyu Guan |
| **Test Engineer** | Write tests, execute, report | Xiyu Guan |
| **Developer** | Fix bugs, maintain tests | Xiyu Guan |
| **CI/CD Owner** | Maintain pipeline, automation | Xiyu Guan |

---

## Success Criteria

### For Individual Test Cycles
- âœ… â‰¥ 90% of selected tests pass
- âœ… All critical bugs fixed
- âœ… Test report completed with evidence
- âœ… Code coverage meets target
- âœ… No regression in previously passing tests

### For Project Release
- âœ… All test cycles completed
- âœ… Overall code coverage â‰¥ 70%
- âœ… Zero critical/high priority bugs open
- âœ… All security tests passed
- âœ… Performance benchmarks met
- âœ… Documentation complete and approved

---

## Future Improvements

### Short Term (Next Sprint)
1. Add fixtures for common test setup
2. Implement test result dashboard
3. Add performance benchmarking
4. Create test data repository

### Long Term
1. Visual regression testing for UI
2. Load testing with realistic traffic
3. Automated accessibility testing
4. Integration with bug tracking system

---

## References

### Internal Documents
- [TEST_CASES.md](TEST_CASES.md) - Comprehensive test case list
- [Test Plans](test_plans/) - Cycle-specific test plans
- [Test Reports](test_reports/) - Test execution results
- [CI Setup](../../CI_SETUP.md) - CI/CD configuration guide

### External Resources
- [pytest Documentation](https://docs.pytest.org/)
- [FastAPI Testing](https://fastapi.tiangolo.com/tutorial/testing/)
- [GitHub Actions](https://docs.github.com/en/actions)

---

## Contact

**For testing questions or issues**:
- **Name**: Xiyu Guan
- **Email**: xiyug@student.unimelb.edu.au
- **Role**: Test Lead & Developer

---

**Document Version**: 1.0  
**Created**: October 25, 2025  
**Next Review**: November 1, 2025  
**Status**: âœ… Active

